{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyX8AHxYhuqMiObU6v9+VE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menhguin/natural_language_rl/blob/main/Another_copy_of_Natural_Language_RL_Common_solver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup 1: Install"
      ],
      "metadata": {
        "id": "S_QURi3BA2u6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yEKSEs-dfAL"
      },
      "outputs": [],
      "source": [
        "\"\"\"NLRL_with_Goodfire.ipynb\n",
        "\n",
        "Natural Language Reinforcement Learning using Goodfire SDK\n",
        "Following the steps outlined in the paper:\n",
        "1. Query Selection & Answer Verification\n",
        "2. COT Decomposition & Critical Token Identification\n",
        "3. Feature Attribution & Analysis\n",
        "4. Feature Steering\n",
        "5. Robustness Testing & Iteration\n",
        "\"\"\"\n",
        "\n",
        "#####################################\n",
        "# Step 0: Setup and Initialization\n",
        "#####################################\n",
        "\n",
        "# Install required packages\n",
        "!pip install goodfire --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install tqdm --quiet\n",
        "\n",
        "from google.colab import userdata\n",
        "import goodfire\n",
        "import asyncio\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "from typing import List\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Example:\n",
        "    query: str\n",
        "    response: str\n",
        "    is_correct: bool\n",
        "\n",
        "# Add your Goodfire API Key to your Colab secrets\n",
        "GOODFIRE_API_KEY = userdata.get('GOODFIRE_API_KEY')\n",
        "\n",
        "\n",
        "# Initialize SDK clients\n",
        "client = goodfire.Client(GOODFIRE_API_KEY)\n",
        "async_client = goodfire.AsyncClient(GOODFIRE_API_KEY)\n",
        "\n",
        "# Instantiate a model variant\n",
        "variant = goodfire.Variant(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# for my reference: Which is bigger, 9.9 or 9.11?\n",
        "# https://platform.goodfire.ai/chat/ab5cc226-aa37-404c-87fd-4fb7949944c7\n",
        "# https://platform.goodfire.ai/chat/0a670b58-0b76-44c1-a7cb-8356347d83c9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup 2: Load dataset"
      ],
      "metadata": {
        "id": "x5myZzGjtESg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# Cell 1: Load and Prepare Dataset\n",
        "#####################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load all datasets\n",
        "dataset = load_dataset(\"anishthalamati/nyt-connections\")\n",
        "train_set = dataset['train']\n",
        "validation_set = dataset['validation']\n",
        "test_set = dataset['test']\n",
        "\n",
        "# Print sample from each dataset to verify structure\n",
        "print(\"\\nTrain set sample:\")\n",
        "print(train_set[0])\n",
        "print(f\"Train set size: {len(train_set)}\")\n",
        "\n",
        "print(\"\\nValidation set sample:\")\n",
        "print(validation_set[0])\n",
        "print(f\"Validation set size: {len(validation_set)}\")\n",
        "\n",
        "print(\"\\nTest set sample:\")\n",
        "print(test_set[0])\n",
        "print(f\"Test set size: {len(test_set)}\")"
      ],
      "metadata": {
        "id": "wSNtEEoNdmzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Autosteer"
      ],
      "metadata": {
        "id": "PuCnSMrQtMvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# Part 1: Skill Discovery\n",
        "#####################################\n",
        "\n",
        "class ExampleManager:\n",
        "    \"\"\"Manages and filters training examples\"\"\"\n",
        "\n",
        "    def __init__(self, train_set, validation_set, test_set):\n",
        "        self.train_set = train_set\n",
        "        self.validation_set = validation_set\n",
        "        self.test_set = test_set\n",
        "        print(f\"\\nInitialized ExampleManager with:\")\n",
        "        print(f\"- {len(train_set)} training examples\")\n",
        "        print(f\"- {len(validation_set)} validation examples\")\n",
        "        print(f\"- {len(test_set)} test examples\")\n",
        "\n",
        "        # Debug: Print structure of first example\n",
        "        print(\"\\nExample data structure:\")\n",
        "        example = train_set[0]\n",
        "        print(json.dumps(example, indent=2))\n",
        "\n",
        "    def prepare_examples(self, dataset, max_examples: int = 100) -> List[Example]:\n",
        "        \"\"\"Convert dataset examples into Example objects\"\"\"\n",
        "        print(f\"\\nPreparing up to {max_examples} examples...\")\n",
        "        examples = []\n",
        "\n",
        "        # Convert to list to use slicing\n",
        "        dataset_subset = dataset.select(range(min(max_examples, len(dataset))))\n",
        "\n",
        "        for i, item in enumerate(tqdm(dataset_subset, desc=\"Converting examples\")):\n",
        "            # Print first few examples to verify conversion\n",
        "            if i < 3:\n",
        "                print(f\"\\nExample {i + 1}:\")\n",
        "                print(f\"Words: {item['text']}\")\n",
        "                print(f\"Category: {item['label']}\")\n",
        "\n",
        "            example = Example(\n",
        "                query=f\"puzzle: Consider these words and explain how they might be related: {item['text']}\",\n",
        "                response=f\"These words belong to the category: {item['label']}\",\n",
        "                is_correct=True\n",
        "            )\n",
        "            examples.append(example)\n",
        "\n",
        "            if i == 3:\n",
        "                print(\"\\n... (continuing conversion)\")\n",
        "\n",
        "        print(f\"\\nPrepared {len(examples)} examples total\")\n",
        "        return examples\n",
        "\n",
        "class SkillDiscoverer:\n",
        "    \"\"\"Discovers reasoning skills using AutoSteer\"\"\"\n",
        "\n",
        "    def __init__(self, client, variant):\n",
        "        self.client = client\n",
        "        self.variant = variant\n",
        "        self.discovered_skills = None\n",
        "        self.activation_logs = {}\n",
        "\n",
        "    async def discover_skills(self, correct_examples: List[Example], incorrect_examples: List[Example], top_k: int = 5):\n",
        "        \"\"\"Use AutoSteer to discover skills that distinguish correct from incorrect examples\"\"\"\n",
        "        print(\"\\n=== Skill Discovery and Analysis ===\")\n",
        "\n",
        "        # Convert examples to message format\n",
        "        print(\"\\nFormatting examples for contrast...\")\n",
        "        print(\"\\nCorrect examples:\")\n",
        "        correct_messages = []\n",
        "        for i, ex in enumerate(correct_examples):\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": ex.query},\n",
        "                {\"role\": \"assistant\", \"content\": ex.response}\n",
        "            ]\n",
        "            correct_messages.append(messages)\n",
        "            if i < 3:  # Show first 3 examples\n",
        "                print(f\"\\nExample {i+1}:\")\n",
        "                print(f\"Query: {ex.query}\")\n",
        "                print(f\"Response: {ex.response}\")\n",
        "\n",
        "        print(\"\\nIncorrect examples:\")\n",
        "        incorrect_messages = []\n",
        "        for i, ex in enumerate(incorrect_examples):\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": ex.query},\n",
        "                {\"role\": \"assistant\", \"content\": ex.response}\n",
        "            ]\n",
        "            incorrect_messages.append(messages)\n",
        "            if i < 3:  # Show first 3 examples\n",
        "                print(f\"\\nExample {i+1}:\")\n",
        "                print(f\"Query: {ex.query}\")\n",
        "                print(f\"Response: {ex.response}\")\n",
        "\n",
        "        # Original skill discovery\n",
        "        print(f\"\\nFinding contrasting features between {len(correct_messages)} correct and {len(incorrect_messages)} incorrect examples...\")\n",
        "        _, skill_features = self.client.features.contrast(\n",
        "            dataset_1=incorrect_messages,\n",
        "            dataset_2=correct_messages,\n",
        "            model=self.variant,\n",
        "            top_k=top_k*2\n",
        "        )\n",
        "        print(f\"\\nFound {len(skill_features)} initial features:\")\n",
        "        for i, feat in enumerate(skill_features):\n",
        "            print(f\"{i+1}. {feat.label}\")\n",
        "\n",
        "        # Rerank for task relevance\n",
        "        print(\"\\nReranking features...\")\n",
        "        self.discovered_skills = self.client.features.rerank(\n",
        "            features=skill_features,\n",
        "            query=\"pattern recognition and categorization for word puzzles\",\n",
        "            model=self.variant,\n",
        "            top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Analyze example activations\n",
        "        print(\"\\n=== Feature Activation Analysis ===\")\n",
        "        for i, ex in enumerate(tqdm(correct_examples + incorrect_examples, desc=\"Analyzing examples\")):\n",
        "            context = self.client.features.inspect(\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": ex.query},\n",
        "                    {\"role\": \"assistant\", \"content\": ex.response}\n",
        "                ],\n",
        "                model=self.variant,\n",
        "                features=self.discovered_skills\n",
        "            )\n",
        "\n",
        "            activations = context.top(k=len(self.discovered_skills))\n",
        "\n",
        "            # Store activation data\n",
        "            self.activation_logs[i] = {\n",
        "                'query': ex.query,\n",
        "                'response': ex.response,\n",
        "                'is_correct': i < len(correct_examples),\n",
        "                'activations': [(act.feature.label, float(act.activation)) for act in activations]\n",
        "            }\n",
        "\n",
        "            # Print analysis for this example\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Query: {ex.query[:100]}...\")\n",
        "            print(f\"{'Correct' if i < len(correct_examples) else 'Incorrect'} example\")\n",
        "            print(\"Top Feature Activations:\")\n",
        "            for feat, strength in sorted(self.activation_logs[i]['activations'],\n",
        "                                      key=lambda x: abs(x[1]),\n",
        "                                      reverse=True):\n",
        "                print(f\"  {feat}: {strength:.3f}\")\n",
        "\n",
        "        # Calculate feature statistics\n",
        "        print(\"\\n=== Feature Importance Summary ===\")\n",
        "        feature_stats = {}\n",
        "        for feat in self.discovered_skills:\n",
        "            activations = []\n",
        "            correct_activations = []\n",
        "            incorrect_activations = []\n",
        "\n",
        "            for log in self.activation_logs.values():\n",
        "                for f, strength in log['activations']:\n",
        "                    if f == feat.label:\n",
        "                        activations.append(strength)\n",
        "                        if log['is_correct']:\n",
        "                            correct_activations.append(strength)\n",
        "                        else:\n",
        "                            incorrect_activations.append(strength)\n",
        "\n",
        "            avg_activation = np.mean(activations) if activations else 0\n",
        "            avg_correct = np.mean(correct_activations) if correct_activations else 0\n",
        "            avg_incorrect = np.mean(incorrect_activations) if incorrect_activations else 0\n",
        "\n",
        "            print(f\"\\nFeature: {feat.label}\")\n",
        "            print(f\"Average activation: {avg_activation:.3f}\")\n",
        "            print(f\"Average for correct examples: {avg_correct:.3f}\")\n",
        "            print(f\"Average for incorrect examples: {avg_incorrect:.3f}\")\n",
        "            print(f\"Activation difference: {(avg_correct - avg_incorrect):.3f}\")\n",
        "\n",
        "        return self.discovered_skills\n",
        "\n",
        "\n",
        "#####################################\n",
        "# Main Pipeline\n",
        "#####################################\n",
        "\n",
        "async def run_skill_discovery(num_samples: int = 20):\n",
        "    \"\"\"Run the skill discovery part and return both skills and the discoverer instance\"\"\"\n",
        "    print(f\"\\n=== Starting Skill Discovery Pipeline (using {num_samples} samples) ===\")\n",
        "\n",
        "    # Initialize components\n",
        "    print(\"\\nInitializing components...\")\n",
        "    example_manager = ExampleManager(train_set, validation_set, test_set)\n",
        "    skill_discoverer = SkillDiscoverer(client, variant)\n",
        "\n",
        "    # Prepare training examples\n",
        "    train_examples = example_manager.prepare_examples(train_set, max_examples=num_samples)\n",
        "\n",
        "    # Split examples\n",
        "    mid_point = len(train_examples) // 2\n",
        "    correct_examples = train_examples[:mid_point]\n",
        "    incorrect_examples = train_examples[mid_point:]\n",
        "\n",
        "    print(f\"\\nSplit examples into {len(correct_examples)} correct and {len(incorrect_examples)} incorrect\")\n",
        "\n",
        "    # Discover skills\n",
        "    skills = await skill_discoverer.discover_skills(correct_examples, incorrect_examples)\n",
        "\n",
        "    print(\"\\nDiscovered Skills:\")\n",
        "    for i, skill in enumerate(skills, 1):\n",
        "        print(f\"{i}. {skill}\")\n",
        "        print(f\"   Label: {skill.label}\")\n",
        "        print(f\"   UUID: {skill.uuid}\")\n",
        "\n",
        "    print(\"\\n=== Skill Discovery Complete ===\")\n",
        "\n",
        "    return skills, skill_discoverer  # Return both\n",
        "\n",
        "# Run first step\n",
        "if __name__ == \"__main__\":\n",
        "    num_samples = 20  # Adjust this number as needed\n",
        "    discovered_skills, skill_discoverer = await run_skill_discovery(num_samples)\n",
        ""
      ],
      "metadata": {
        "collapsed": true,
        "id": "yg06TM5XsxIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# Step 3: Visualization and Analysis\n",
        "#####################################\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def create_analysis_dataframe(activation_logs):\n",
        "    \"\"\"Convert activation logs to a pandas DataFrame for analysis\"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for example_id, log in activation_logs.items():\n",
        "        # For each feature activation in this example\n",
        "        for feature, activation in log['activations']:\n",
        "            rows.append({\n",
        "                'example_id': example_id,\n",
        "                'query': log['query'],\n",
        "                'response': log['response'],\n",
        "                'is_correct': log['is_correct'],\n",
        "                'feature': feature,\n",
        "                'activation': activation\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def visualize_results(skill_discoverer):\n",
        "    \"\"\"Create visualizations and interactive tables from results\"\"\"\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = create_analysis_dataframe(skill_discoverer.activation_logs)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('feature_analysis.csv', index=False)\n",
        "    print(\"Saved detailed results to feature_analysis.csv\")\n",
        "\n",
        "    # 1. Feature Activation Heatmap\n",
        "    pivot_df = df.pivot_table(\n",
        "        values='activation',\n",
        "        index='example_id',\n",
        "        columns='feature',\n",
        "        aggfunc='first'\n",
        "    )\n",
        "\n",
        "    fig1 = px.imshow(\n",
        "        pivot_df,\n",
        "        title='Feature Activation Heatmap',\n",
        "        labels=dict(x='Feature', y='Example ID', color='Activation Strength'),\n",
        "        aspect='auto'\n",
        "    )\n",
        "    fig1.show()\n",
        "\n",
        "    # 2. Feature Importance Bar Chart\n",
        "    avg_activations = df.groupby('feature')['activation'].agg(['mean', 'std']).reset_index()\n",
        "    fig2 = px.bar(\n",
        "        avg_activations,\n",
        "        x='feature',\n",
        "        y='mean',\n",
        "        error_y='std',\n",
        "        title='Average Feature Activation Strength',\n",
        "        labels={'mean': 'Average Activation', 'feature': 'Feature'}\n",
        "    )\n",
        "    fig2.update_layout(xaxis_tickangle=45)\n",
        "    fig2.show()\n",
        "\n",
        "    # 3. Interactive Table\n",
        "    def generate_interactive_table(df):\n",
        "        # Create unique example IDs for filtering\n",
        "        examples = df[['example_id', 'query', 'response', 'is_correct']].drop_duplicates()\n",
        "        features = df['feature'].unique()\n",
        "\n",
        "        # Create HTML for filtering controls\n",
        "        filter_html = f\"\"\"\n",
        "        <div style=\"margin-bottom: 20px;\">\n",
        "            <h3>Filters:</h3>\n",
        "            <select id=\"example_filter\" onchange=\"filterTable()\">\n",
        "                <option value=\"all\">All Examples</option>\n",
        "                {''.join(f'<option value=\"{i}\">Example {i+1}</option>' for i in examples['example_id'])}\n",
        "            </select>\n",
        "\n",
        "            <select id=\"feature_filter\" onchange=\"filterTable()\">\n",
        "                <option value=\"all\">All Features</option>\n",
        "                {''.join(f'<option value=\"{f}\">{f}</option>' for f in features)}\n",
        "            </select>\n",
        "\n",
        "            <select id=\"correct_filter\" onchange=\"filterTable()\">\n",
        "                <option value=\"all\">All Results</option>\n",
        "                <option value=\"correct\">Correct Only</option>\n",
        "                <option value=\"incorrect\">Incorrect Only</option>\n",
        "            </select>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Create table\n",
        "        table_html = df.to_html(classes='data-table', index=False)\n",
        "\n",
        "        # Add styling and JavaScript\n",
        "        return f\"\"\"\n",
        "        <style>\n",
        "            .data-table {{\n",
        "                width: 100%;\n",
        "                border-collapse: collapse;\n",
        "            }}\n",
        "            .data-table th, .data-table td {{\n",
        "                padding: 8px;\n",
        "                border: 1px solid #ddd;\n",
        "            }}\n",
        "            .data-table tr:nth-child(even) {{\n",
        "                background-color: #f9f9f9;\n",
        "            }}\n",
        "            .filter-controls {{\n",
        "                margin-bottom: 20px;\n",
        "            }}\n",
        "            select {{\n",
        "                margin-right: 10px;\n",
        "                padding: 5px;\n",
        "            }}\n",
        "        </style>\n",
        "        {filter_html}\n",
        "        {table_html}\n",
        "        <script>\n",
        "        function filterTable() {{\n",
        "            var example = document.getElementById('example_filter').value;\n",
        "            var feature = document.getElementById('feature_filter').value;\n",
        "            var correct = document.getElementById('correct_filter').value;\n",
        "\n",
        "            var rows = document.querySelectorAll('.data-table tbody tr');\n",
        "            rows.forEach(function(row) {{\n",
        "                var showRow = true;\n",
        "\n",
        "                if (example !== 'all' && row.cells[0].textContent !== example) showRow = false;\n",
        "                if (feature !== 'all' && row.cells[4].textContent !== feature) showRow = false;\n",
        "                if (correct !== 'all') {{\n",
        "                    var isCorrect = row.cells[3].textContent === 'True';\n",
        "                    if (correct === 'correct' && !isCorrect) showRow = false;\n",
        "                    if (correct === 'incorrect' && isCorrect) showRow = false;\n",
        "                }}\n",
        "\n",
        "                row.style.display = showRow ? '' : 'none';\n",
        "            }});\n",
        "        }}\n",
        "        </script>\n",
        "        \"\"\"\n",
        "\n",
        "    # Display interactive table\n",
        "    display(HTML(generate_interactive_table(df)))\n",
        "\n",
        "# Usage (add to main):\n",
        "print(\"\\nGenerating visualizations...\")\n",
        "visualize_results(skill_discoverer)"
      ],
      "metadata": {
        "id": "SmmWdNgLIeoO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}